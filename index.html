<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>3D Detection and Characterization of Sources in ALMA Data through Deep Learning</title>
		<meta name="description" content="AI Forum June 01 2022">
		<link rel="stylesheet" href="reveal.js/dist/reset.css">
		<link rel="stylesheet" href="reveal.js/dist/reveal.css">
		<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css", id="theme">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- First slice - Title -->
				<section data-background-iframe="background.html">
					<div class="container">
						<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4)">
							<h1>
								3D Detection and Characterization of Sources in ALMA Data through Deep Learning
							</h1>
						</div>
					</div>
					<hr>
					<div style="border-radius: 20px; background-color:rgba(0,0, 0, 0)">
						<div class="container">
							<div class="col">
								<div align="left" style="margin-left: 20px">
									<h2>
										Michele Delli Veneri
										<br>
										<img src="assets/logo_infn.png" class="plain" 
										      height="100" style="background-color:rgb(253, 253, 253)"><img>
										<br>
									</h2>
								</div>
							</div>

							<div align="right" class="col">
								<br>
								<br>
								<img src="assets/logo_unina.png" class="plain" 
								     style="background-color:rgb(253, 253, 253)" height="100"><img>
								<br>
							</div>
						</div>
						<p style="color: rgb(41, 230, 255); font-size:18px">In collaboration with: Lukasz Tychoniec (ESO), Fabrizia Guglielmetti (ESO), Eric Villard (ESO), Giuseppe Longo (UNINA)</p>
						<div> slides at <a href="http://MicheleDelliVeneri.github.io/AIForum">MicheleDelliVeneri.github.io/AIForum </a></div>
					</div>
				</section>
				<!-- Second set of slices - Our Motivation and what we are trying to achieve -->
				<section>
					<section>
						<h2 class="slide-title" style="position:absolut; top:0;"> Our Motivations: the road to ALMA 2030</h2>
						<ul align="left">
							<li class="fragment fade-up"> ALMA new correlators will  
								<b class="alert">increase</b> the bandwith </li>
							<li class="fragment fade-up">$\Longrightarrow$ with a consequential improvement in daily 
								<b class="alert">data rate</b> </li>
							<li class="fragment fade-up"> $\Longrightarrow$ and thus with a consequential 
								increse in <b class="alert">data size</b> </li>
						</ul>
						<br>
						<br>
						<div class="fragment fade-up">
							ALMA will transition from its current TB data regime to a
							<h3> <b class="alert">PB data regime</b></h3>
						</div>
						<p class="fragment fade-down">$\big\Downarrow$</p>
						<div class="fragment fade-up">
							Current solutions are insufficient to deal with this data transition 
							and several issues are still to be addressed:
						</div>
						<br>
						<div align="left" style="margin-left: 40px">
							<ul >
								<li class="fragment fade-up">Data Processing Speed</li>
								<li class="fragment fade-up">Data Calibration</li>
								<li class="fragment fade-up">Source Detection</li>
								<li class="fragment fade-up">Dependencies from simplistic models:<br>
									&nbsp &nbsp atmosphere, primary and dirty beam, sources morphologies, nature of noise, array configuration</li>
								<li class="fragment fade-up">Data Storage and Distribution</li>
							</ul>
						</div>
						<p class="fragment fade-down">$\big\Downarrow$</p>
						<h3 class="fragment fade-down"> <b class="alert"> Machine Learning </b> </h3>
					</section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">What we are trying to achieve</h3>
						<ul class="fragment fade-down">
							<li>Increase the Image Processing Speed</li>
							<li>Make as few assumptions as possible, and learn more from the data</li>
							<li>Completely automatize the source detection and characterization</li>
							<li>Data compression rates allowing storage
								 and distribution of data with the <b class="alert">current infrastructures</b>.</li>
						</ul>
						<div class="fragment fade-up" style="position:absolute; margin:0 auto">
							<img src="assets/scope.png" height="150"/>
						</div>

						<div class="fragment fade-up">
							<br>
							<br>
							<br>
							<br>
							<br>
							<br>
							<br>

							<h3 style="font-size:24px"><b class="alert">Are we the first ones that try to do that?</b></h3>
							<ul>
								<li class="fragment fade-up" style="font-size:20px">
									<b>Lanusse</b> et al. (2020) Deep Generative Models for Galaxy Image Simulations 
									<a href="https://arxiv.org/abs/2008.03833">
										<img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg" 
										class="plain" style="height:20px;vertical-align:middle;" /></a></li>
								<li class="fragment fade-up" style="font-size:20px">
									<b>Schmidt</b> et al. (2022) Deep Learning-based Imaging in Radio Interferometry 
									<a href="https://arxiv.org/abs/2203.11757">
										<img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2203.11757-B31B1B.svg" 
										class="plain" style="height:20px;vertical-align:middle;" /></a></li>
								<li class="fragment fade-up" style="font-size:20px">
									<b>Rezaei</b> et al. (2021) DECORAS: detection and characterization of radio-astronomical sources using deep learning 
									<a href="https://arxiv.org/abs/2109.09077">
										<img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2109.09077-B31B1B.svg" 
										class="plain" style="height:20px;vertical-align:middle;" /></a></li>
							
							</ul>
						</div>
						<div class="fragment fade-up">
							<h3 style="font-size:24px">So how are we <b class="alert">different</b>?</h3>
							<ul>
								<lic class="fragment fade-up">We do not only work on 2D images, but on <b class="alert">cubes</b> 
									because integrating it's not always the best solution!</lic>
							</ul>
						</div>

					</section>
				</section>
			<!-- Third set of slices - Iamge Denoising and Deconvolution with Deeep Learning -->
			<!--			
			<section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">The Classical point of view</h3>
						<div class="container">
							<div class="col">
								<img class="plain fragment" data-src="assets/rand_z_square.png" style="height: 150px" data-fragment-index="4"/>
							</div>
							<div class="col">
								<img class="plain fragment" data-src="assets/cosmos_gal.png" style="width: 200px" data-fragment-index="3"/>
							</div>
							<div class="col">
								<img class="plain fragment" data-src="assets/cosmos_gal_psf.png" style="width: 200px" data-fragment-index="2"/>
							</div>

							<div class="col">
								<img class="plain fragment" data-src="assets/cosmos_gal_pix.png" style="width: 200px" data-fragment-index="1"/>
							</div>

							<div class="col">
								<img class="plain fragment" data-src="assets/cosmos_gal_ground.png" style="width: 200px" data-fragment-index="0"/>
							</div>
						</div>

						<div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
							<div class='col fragment' data-fragment-index='4'> <font size="10"> $\longrightarrow$ </font> <br> $g_\theta$ </div>
							<div class='col fragment' data-fragment-index='3'> <font size="10"> $\longrightarrow$ </font> <br> PSF </div>
							<div class='col fragment' data-fragment-index='2'> <font size="10"> $\longrightarrow$ </font> <br> Pixelation</div>
							<div class='col fragment' data-fragment-index='1'> <font size="10"> $\longrightarrow$ </font> <br> Noise and uv map </div>
						</div>

						<div class="container">
							<div class="col">
								<div style="position:relative; width:400px; height:300px; margin:0 auto;">
									<img data-src="assets/pgm_0.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="0"/>
									<img data-src="assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="1"/>
									<img data-src="assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="2"/>
									<img data-src="assets/pgm_2.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="3"/>
									<img data-src="assets/pgm_3.png" class="plain fragment " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="4"/>
								</div>
							</div>

							<div class=" col">
								<div class="block fragment" data-fragment-index="0">
									<div class="block-title">
								 		Probabilistic model
									</div>
								
									<div class="block-content">
										<div style="position:relative; width:400px; height:100px; margin:0 auto; font-size:20px">
											<div class="plain fragment current-visible " 
								    			style="position:absolute;top:0;left:0;width:400px;" 
												data-fragment-index="0"> $$ x \sim ? $$ 
											</div>
								  			<div class="plain fragment current-visible " 
								    			style="position:absolute;top:0;left:0;width:400px;" 
												data-fragment-index="1"> $$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br>latent $z$ is a denoised galaxy image
											</div>
								  			<div class="plain fragment current-visible " 
								    			style="position:absolute;top:0;left:0;width:400px;" 
												data-fragment-index="2"> $$ x \sim \mathcal{N}( \mathbf{P} z, \Sigma) \quad z \sim ?$$<br>latent $z$ is a super-resolved and denoised galaxy image
											</div>
								  			<div class="plain fragment current-visible " 
								    			style="position:absolute;top:0;left:0;width:400px;" 
												data-fragment-index="3"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast z), \Sigma) \quad z \sim ? $$<br>latent $z$ is a deconvolved, super-resolved, and denoised galaxy image 
											</div>
								  			<div class="plain fragment " 
											  	style="position:absolute;top:0;left:0;width:400px;" 
												data-fragment-index="4"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast g_\theta(z)), \Sigma) \quad z \sim \mathcal{N}(0, \mathbf{I}) $$ <br>latent $z$ is a Gaussian sample<br> <b class="alert"> $\theta$ are parameters of the model</b> 
											</div>
										</div>
										<br>
										<br>
										<br>
									</div>
								</div>
							</div>
						</div>
						<div class="fragment"> 
							$\Longrightarrow$ Standard methods have to make assumptions to make this steps.
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Image Denoising and Deconvolution with Deep Learning</h3>
						<br>
			    		<br>
			    		$ y = P \ast x + n $
			    		<div class="container">
			    			<div class="col">
			    				<p> <b class="alert"> Observed $y$</b></p>
			    				<img class="plain" data-src="assets/cosmos_gal_ground.png" style="width: 250px" />
			    				<br>Ground-Based Telescope
			    			</div>
			    			<div class="col fragment fade-up" data-fragment-index='0'>
			    				<p> <b class="alert">$f_\theta$</b> </p>
			    				<img class="plain " data-src="assets/generic_network_inv.png" style="height: 250px; width:500px" />
			    				<br>some deep Convolutional Neural Network
			    			</div>
			    			<div class="col">
			    				<p><b class="alert"> Unknown $x$</b> </p>
			    				<img class="plain" data-src="assets/cosmos_gal.png" style="width: 250px" />
			    				<br>Hubble Space Telescope
			    			</div>
			    		</div>
			    		<br>
			    		<br>
			    		<ul>
			    			<li class="fragment fade-up" data-fragment-index='0'> The objective is to train a neural network $f_\theta$ to <b class="alert">estimate $x$ given $y$</b>.
			    			</li>
			    		</ul>
					</section>

					<section>
						<ul>
							<li> <i>Step I</i>: Get your hands on a <b class="alert">training set</b>  of images (<b>data</b> or <b>simulations</b>)
								$$\mathcal{D} = \{(x_0, y_0),
								(x_1, y_1), \ldots, (x_N, y_N) \}$$
								$\Longrightarrow$ realize that the data actually contains <b class="alert">hardcoded assumptions</b> about PSF $P$
								noise $n$, and galaxy morphology $x$.
							</li>
							<li class="fragment fade-up"> Step II: Train the neural network $f_\theta$ under the <b class="alert">regression loss</b> of your choice,
								for example:
								$$ \mathcal{L} = \sum_{i=0}^N \parallel x_i - f_\theta(y_i)\parallel^2 $$
							</li>
						</ul>
						<div class="container fragment">
							<div class="col">
								<img class="plain" data-src="assets/cosmos_gal_ground.png" style="width: 250px" />
								<p>$$ y $$</p>
							</div>

							<div class="col">
								<img class="plain " data-src="assets/generic_network_inv.png" style="height: 250px; width:500px" />
								<p>$$f_\theta$$</p>
							</div>

							<div class="col">
								<img class="plain" data-src="assets/rec_median.png" style="width: 250px" />
								$$f_\theta(y)$$
							</div>

							<div class="col fragment fade-up" style="float:center;">
								<img class="plain" data-src="assets/cosmos_gal.png" style="width: 250px" />
								<br>
								<p>$$ \mbox{True } x $$</p>
							</div>
						</div>
						<div class="fragment">$\Longrightarrow$Why is the network output different from the truth? If it's not the truth, then <b>what is $f_\theta(y)$?</b></div>
					</section>

					<section>
						<p>Let's take a second look at the <b class="alert">loss function</b></p>
						$$ \mathcal{L} = \sum_{(x_i, y_i) \in \mathcal{D}} \parallel x_i - f_\theta(y_i)\parallel^2 \quad \simeq \quad \int \parallel x - f_\theta(y) \parallel^2 \ p(x,y) \ dx dy $$

						<div class="fragment" data-fragment-index="1">$$\Longrightarrow \int \left[ \int \parallel x - f_\theta(y) \parallel^2 \ p(x|y) \ dx \right] p(y) dy $$ </div>

						<div class="block fragment" data-fragment-index="2", style="font-size:22px">
							<div class="block-content">
								$\mathcal{L}$ minimized when $f_{\theta^\star}(y) = \int x \ p(x|y) \ dx $, i.e.
								when <b class="alert">$f_{\theta^\star}(x)$ is predicting the mean of $p(x|y)$</b>.
							</div>
						</div>
						<div class="container" style="font-size:20px">
							<div class="col">
								<div style="position:relative; width:500px; height:500px; margin:0 auto;">
									<img class="fragment current-visible plain" data-src="assets/nn_l2.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
									<img class="fragment current-visible plain" data-src="assets/nn_l2_mean.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="4" />
									<img class="fragment current-visible plain" data-src="assets/nn_l1.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="5" />
									<img class="fragment plain" data-src="assets/nn_l1_median.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="6" />
								</div>
							</div>
							<div class="col">
								<ul>
									<li class="fragment" data-fragment-index="3"> Using an <b class="alert">$\ell_2$ loss learns the mean</b> of the $p(x | y)$
									</li>
									<br>
									<li class="fragment" data-fragment-index="5"> Using an <b class="alert">$\ell_1$ loss learns the median</b> of $p(x|y)$
									</li>
									<br>
									<li class="fragment" data-fragment-index="7"> It would be <b>exceptional</b> if you could capture morphological nuances by aiming to learn their mean or median.<br>
									</li>
								</ul>
							</div>
						</div>
					</section>

					<section>
						<div class="fragment fade-up">
							<q>The network is exactly learning what it is supposed to be learning which
							differs from what you may think that the network is trying to learn.</q>
							<br>
							<q class="fragment fade-up" align="left">...... and that's bad, very bad</q>
							<p class="fragment fade-up" align="left"><b class="alert">Myself after too many drinks in Garching</b></p>	
							
						</div>
					   
					</section>
				</section>


-->

				<!-- Fourth set of slices - Simulating ALMA Observations -->
				<section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Simulating Sky Models</h3>
						<div class="container" style="font-size: 20px">
							<div class="fragment fade-up" align="left" style="margin-left:40px">
								<ul>
									<li>Our main objective is to have in the same skymodel bright sources and fainter ones</li>
									<li>We generate a random number $n \in [2, 5]$ 2D Gaussian components</li>
									<li>$FWHM_{s} \in [2, 8]$ pixels</li>
									<li>$\theta \in [0^{\circ}, 90^{\circ}]$</li>
									<li>Brightness $\in [1, 5]$ flux units</li>
								</ul>
							</div>
						</div>
						<dev class="fragment fade-up">
							<h4 data-id="code-title">Model Simulation Code</h>
						</dev>
						<div class="container">
							<pre data-id="code-animation">
								<code  data-trim data-line-numbers=1-30|32-57>
									def make_cube(i, amps, xyposs, fwhms, angles,
									              line_centres, line_fwhms, idxs, z_idxs):
									    number_of_components = random.randint(2,5)
									    lines = []
									    images = []
									    params = []
									    for _ in range(number_of_components):
									        # Random choice of line parameters
									        line_amp = np.random.choice(amps)
									        line_cent = np.random.choice(line_centres)
									        line_fwhm = np.random.choice(line_fwhms)
									        lines.append(gaussian(z_idxs, line_amp, line_cent, line_fwhm))

									        # Random choice of source parameters
									        amp = np.random.choice(amps)
									        pos_x =  np.random.choice(xyposs)
									        pos_y =  np.random.choice(xyposs)
									        fwhm_x = np.random.choice(fwhms)
									        fwhm_y = np.random.choice(fwhms)
									        pa = np.random.choice(angles)
									        images.append(twodgaussian(amp, pos_x, pos_y, fwhm_x, fwhm_y, pa, idxs))
									        params.append([int(i), round(amp, 2), round(pos_x, 2), round(pos_y, 2), 
									                       round(fwhm_x, 2), round(fwhm_y, 2), round(pa, 2), round(line_amp, 2), 
									                       round(line_fwhm, 2), round(line_cent, 2)])
									    image = np.sum(images, axis=0)
									    cube = np.ones([128, 350, 350]) * image
									    for z in range(cube.shape[0]):
									        for l in range(len(lines)):
									            cube[z, :, :] += lines[l][z] * images[l]
									    return cube, params

									def generate_cubes(data_dir, csv_name, n):
									    columns = ['ID', 'amplitude', 'x', 'y', 'width_x', 
									               'width_y', 'angle', 'line_peak', 'line_fwhm', 'z']
									    if not os.path.exists(data_dir):
									        os.mkdir(data_dir)
									    amps = np.linspace(1.,5.,num=100).astype(np.float)
									    xyposs = np.arange(100,250).astype(np.float)
									    fwhms = np.linspace(2.,8.,num=100).astype(np.float)
									    angles = np.linspace(0,90,num=900).astype(np.float)
									    line_centres = np.linspace(20, 100, num=100).astype(np.float)
									    line_fwhms = np.linspace(3, 10, num=100).astype(np.float)
									    idxs = np.indices([350, 350])
									    z_idxs = np.linspace(0, 128, 128)
									    parameters = []
									    print('Generating Cubes....')
									    for i in tqdm(range(n)):
									        cube, params = make_cube(i, amps, xyposs, fwhms, angles, line_centres, line_fwhms, idxs, z_idxs)
									        hdu = fits.PrimaryHDU(data=cube)
									        hdu.writeto(data_dir + '/gauss_cube_' + str(i) + '.fits', overwrite=True)
									        for par in params:
									            parameters.append(par)
									    parameters = np.array(parameters)
									    df = pd.DataFrame(parameters, columns=columns)
									    df.to_csv(os.path.join(data_dir, csv_name), index=False)
									    print('Finished!')
								</code>
							</pre>
						</div>	
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Examples of Stacked Simulated Model Cubes</h3>

						<div class="container">
							<div class="col">
								<img data-src="assets/model_image_4.png">
								<img data-src="assets/model_image_9.png">
							</div>
							<div class="col">
								<img data-src="assets/model_image_5.png">
								<img data-src="assets/model_image_12.png">
							</div>
							<div class="col">
								<img data-src="assets/model_image_6.png">
								<img data-src="assets/model_image_15.png">
							</div>
							<div class="col">
								<img data-src="assets/model_image_7.png">
								<img data-src="assets/model_image_17.png">
							</div>
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">It's time to make simulated ALMA Cubes</h3>
						
						<div class="container">
							<div class="col">
								<div class="fragment fade-in">
									<pre data-id="code-animation">
										<code data-trim data-line-numbers="4, 8, 10,12, 13, 14, 15, 16| 29-31">
											def generate_sims(i, input_dir, output_dir):
												filename = os.path.join(input_dir, "gauss_cube_{}.fits".format(str(i)))
												project = "gauss_cube_sim_" + str(i)
												simalma(
													project=project,
													dryrun=False,
													skymodel=filename,
													inbright="0.001Jy/pix",
													indirection="J2000 03h59m59.96s -34d59m59.50s",
													incell="0.1arcsec",
													incenter="230GHz",
													inwidth="10MHz",
													antennalist=["alma.cycle5.3.cfg"],
													totaltime="720s",
													pwv=0.8,
													niter=0,
													overwrite=True,
													verbose=False
												)
												exportfits(imagename=project+'/gauss_cube_sim_'+str(i)+'.alma.cycle5.3.noisy.image', 
														   fitsimage=project+'/gauss_cube_sim_'+str(i)+'.dirty.fits')
												exportfits(imagename=project+'/gauss_cube_sim_'+str(i)+'.alma.cycle5.3.skymodel', 
														   fitsimage=project+'/gauss_cube_sim_'+str(i)+'.skymodel.fits')
		
												os.system('cp ' + project + '/gauss_cube_sim_'+str(i)+'.dirty.fits {}/'.format(output_dir))
												os.system('cp ' + project + '/gauss_cube_sim_'+str(i)+'.skymodel.fits {}/'.format(output_dir))
												os.system('rm -r {}'.format(project))
												os.system('rm *.last')
		
											pool = multiprocessing.Pool()
											pool.map(partial(generate_sims, input_dir=input_dir, output_dir=output_dir), indexes)
										</code>
									</pre>
								</div>
							</div>
							<div class="col">
								<img data-src="assets/psf.png" height="200" width="200">
								<img data-src="assets/primary_beam.png" height="200" width="200">
								
							</div>

						</div>	
					</section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Examples of Stacked Simulated Model Cubes</h3>

						<div class="container">
							<div class="col">
								<img data-src="assets/dirty_image_4.png">
								<img data-src="assets/dirty_image_9.png">
							</div>
							<div class="col">
								<img data-src="assets/dirty_image_5.png">
								<img data-src="assets/dirty_image_12.png">
							</div>
							<div class="col">
								<img data-src="assets/dirty_image_6.png">
								<img data-src="assets/dirty_image_15.png">
							</div>
							<div class="col">
								<img data-src="assets/dirty_image_7.png">
								<img data-src="assets/dirty_image_17.png">
							</div>
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Example of a Dirty Cube</h3>
						<img src="assets/dirty_cube.gif"/>
						<div class="fragment fade-in">
							If you look carefully, you can see that there are frequency ranges in which the faint sources 
							<br> <b class='alert'>pop up!</b>
						</div>
					</section>

				</section>
				<!-- Fifth set of slices - The pipeline -->
				<section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">The Full Pipeline</h3>
						<img data-src="assets/Pipeline.png"/>
					</section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">2D Sources Probability Map</h3>
						<img data-src="assets/Pipeline_ProbabilityMap.png"/>
					</section>
					
					
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							2. Feeding the data to <b class="alert">Blobs Finder</b>
						</h3>
						<h4>Preprocessing and Augmentation: probably the <b class='alert'>most important</b> step for any ML model</h4>
						<img data-src="assets/preprocessingBlobsFinder.png"/>
						<p class="fragment fade-up">We want the network to learn a probability map and thus the input should be 
							<b class="alert">probabilistic</b></p>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							<b class="alert">Deep Learning Libraries</b> which one to choose? </b></h3>
						<video 
							muted
							data-autoplay
							src="assets/pytorch_vs_tensorflow.mp4"
							align="left">
						</video>
						<p> 
							Jokes aside, one is pythonic and object oriented, the other not so much....<br>
							you can guess who is who.
						</p>
						
						
					</section>


					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Blobs Finder</h3>
						<p>It's called Blobs Finder because we do not want it to find morphologies but only to find sources</p>
						<p class="fragment fade-up">The model is learning to <b class='alert'>distinguish sources from noise</b> and 
							also learning the UV map and the background pattern.
						</p>
						<img data-src="assets/BlobsFinder.png">
					</section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Getting to know the <b class="alert">Encoder</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/Encoder.png">
							</div>
							<div class="col">
								<pre data-id="code-animation">
									<code  data-trim data-line-numbers="1|5-9|10-16|18-23|25-30|32-37|39-42|43-51">
										class Enc(nn.Module):
											def __init__(self, c_in, c_hidden, act_fn, padding_mode="zeros"):
												super(Enc, self).__init__()
	
												self.act_fn = act_fn
												self.c_in = c_in
												self.c_hidden = c_hidden
												self.padding_mode = padding_mode
	
												self.block1 = nn.Sequential(
	
													nn.Conv2d(c_in, 8, kernel_size=(3, 3), bias=False, stride=2, 
													padding=1, padding_mode=self.padding_mode),
													self.act_fn(),
													nn.BatchNorm2d(8),
												)
	
												self.block2 = nn.Sequential(
													nn.Conv2d(8, 16, kernel_size=(3, 3), bias=False, stride=2, 
													padding=1, padding_mode=self.padding_mode),
													self.act_fn(),
													nn.BatchNorm2d(16),
												)
	
												self.block3 = nn.Sequential(
													nn.Conv2d(16, 32, kernel_size=(3, 3), bias=False, stride=2, 
													padding=1, padding_mode=self.padding_mode),
													self.act_fn(),
													nn.BatchNorm2d(32),
												)
	
												self.block4 = nn.Sequential(
													nn.Conv2d(32, 64, kernel_size=(3, 3), bias=False, stride=2, 
													padding=1, padding_mode=self.padding_mode),
													self.act_fn(),
													nn.BatchNorm2d(64),
												)
	
												self.dense = nn.Sequential(
														nn.Linear(64 * 16 * 16,  self.c_hidden),
													)
	
											def forward(self, x):
												x = self.block1(x)
												x = self.block2(x)
												x = self.block3(x)
												x = self.block4(x)
												x = torch.flatten(x, start_dim=1)
												x = self.dense(x)
												return x
	
									</code>
								</pre>
							</div>
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Getting to know the <b class="alert">Decoder</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/Decoder.png">
							</div>
							<div class="col">
								<pre data-id="code-animation">
									<code  data-trim data-line-numbers="1-16|18|26-28|30|31-38|39-46|47-54|55-61|62-63|64-73">
										class Interpolate(nn.Module):
    										def __init__(self, scale_factor, mode, align_corners=False):
    										    super(Interpolate, self).__init__()
    										    self.interp = F.interpolate
    										    self.scale_factor = scale_factor
    										    self.mode = mode
    										    self.align_corners = align_corners

    										def forward(self, x):
    										    x = self.interp(
    										        x,
    										        scale_factor=self.scale_factor,
    										        mode=self.mode,
    										        align_corners=self.align_corners,
    										    )
    										    return x

										class Dec2D(nn.Module):
    										def __init__(self, c_in, c_hidden, act_fn):
    										    super(Dec2D, self).__init__()

    										    self.c_in = c_in
    										    self.c_hidden = c_hidden
    										    self.act_fn = act_fn

    										    self.dense = nn.Sequential(
    										        nn.Linear(self.c_hidden, 64 * 16 * 16), self.act_fn()
    										    )

    										    self.unflatten = nn.Unflatten(dim=1, unflattened_size=(64, 16, 16))
    										    self.block1 = nn.Sequential(
    										        Interpolate(scale_factor=(2, 2), mode="bilinear"),
    										        nn.ConvTranspose2d(
    										            self.c_in, 32, kernel_size=(3, 3), bias=False, stride=1, padding=1
    										        ),
    										        self.act_fn(),
    										        nn.BatchNorm2d(32),
    										    )
    										    self.block2 = nn.Sequential(
    										        Interpolate(scale_factor=(2, 2), mode="bilinear"),
    										        nn.ConvTranspose2d(
    										            32, 16, kernel_size=(3, 3), bias=False, stride=1, padding=1
    										        ),
    										        self.act_fn(),
    										        nn.BatchNorm2d(16),
    										    )
    										    self.block3 = nn.Sequential(
    										        Interpolate(scale_factor=(2, 2), mode="bilinear"),
    										        nn.ConvTranspose2d(
    										            16, 8, kernel_size=(3, 3), bias=False, stride=1, padding=1
    										        ),
    										        self.act_fn(),
    										        nn.BatchNorm2d(8),
    										    )
    										    self.block4 = nn.Sequential(
    										        Interpolate(scale_factor=(2, 2), mode="bilinear"),
    										        nn.ConvTranspose2d(
    										            8, 1, kernel_size=(3, 3), bias=False, stride=1, padding=1
    										        ),
    										        self.act_fn(),
    										    )
    										    self.out = nn.Sequential(nn.Conv2d(1, 1, kernel_size=1, stride=1), 
														   nn.Sigmoid())

    										def forward(self, x):
    										    x = self.dense(x)
    										    x = self.unflatten(x)
    										    x = self.block1(x)
    										    x = self.block2(x)
    										    x = self.block3(x)
    										    x = self.block4(x)
    										    x = self.out(x)
    										    return x
	
									</code>
								</pre>
							</div>
						</div>
					</section>
						
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Putting it all together and <b class="alert">Training</b>
						</h3>
						<div class="container">
							<div class="col">
								<p>Blobs Finder</p>
								<pre data-id="code-animation">
									<code data-trim data-line-numbers="1|23-24|29-41">
										class AutoEncoder2D(nn.Module):
											def __init__(
												self, c_in, c_latent, c_out, act_fn, experiment, criterion, loss_weights
											):
												super(AutoEncoder2D, self).__init__()

												self.c_in = c_in
												self.c_latent = c_latent
												self.c_out = c_out
												act_fn_name = {
													"tanh": nn.Tanh,
													"relu": nn.ReLU,
													"leaky_relu": nn.LeakyReLU,
													"gelu": nn.GELU,
													"identity": nn.Identity,
													"sigmoid": nn.Sigmoid,
												}
												self.act_fn = act_fn_name[act_fn]
												self.experiment = experiment
												self.criterion = criterion
												self.loss_weights = loss_weights
											
												self.enc = blocks.Enc2D(self.c_in, self.c_latent, self.act_fn)
												self.dec = blocks.Dec2D(self.c_out, self.c_latent, self.act_fn)
											
												self.step = 0
												self._init_params(act_fn)

											def forward(self, x, y):
												lat = self.enc(x)
												out = self.dec(lat)
												if len(self.criterion) > 0:
													loss = 0
													for i in range(len(self.criterion)):
														loss += self.loss_weights[i] * self.criterion[i](out, y)
												else:
													loss = self.criterion(out, y)
												if self.experiment is not None:
													self.experiment.log_metric("loss", loss.item(), step=self.step)
												self.step += 1
												return loss, out
									</code>
								</pre>
							</div>

							<div class="col">
								<p>Training Script</p>
								<pre data-id="code-animation">
									<code data-trim data-line-numbers="|1|2|3-4|7|8|9|10-12|16|17|18|19|21-23|25-27">
										for epoch in range(num_epochs):
											if phase == "train":
												with experiment.train():
													model.train()
													nb = len(dataloaders[phase])
													running_loss = 0.0
													for batch in tqdm(dataloaders[phase]):
														optimizer.zero_grad()
														with torch.set_grad_enabled():
															loss, output = model.forward(input_.cuda(), target_.cuda())
															loss.backward()
															optimizer.step()
														running_loss += loss.item() * input_.size(0)
													epoch_loss = running_loss / len(dataloaders[phase].dataset)
											else:
												with experiment.validate():
													torch.cuda.empty_cache()
													model.eval()
													with torch.no_grad():
														for batch in tqdm(dataloaders[phase]):
															optimizer.zero_grad()
															loss, output = model.forward(input_.cuda(), target_.cuda())
															running_loss += loss.item() * input_.size(0)
													epoch_loss = running_loss / len(dataloaders[phase].dataset)
													if epoch_loss <= best_loss:
														best_loss = epoch_loss
														save_checkpoint(model, optimizer, outpath, epoch)
									</code>
								</pre>	
							</div>
						</div>

					</section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							<b class="alert">Never Forget to save the weights!</b>
						</h3>
						<div class="container">
							<div class="col">
								<video
									muted
									data-autoplay
									src="assets/SaveTheModel.mp4"
									align="left">
								</video>
							</div>
							<div class="col" align="left" style="font-size:18px">
								$SSIM(x, y) = \frac{(2 \mu_x \mu_y + c_1) (2 \sigma_{xy} + c_2)}{(\mu_x^2 \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)}$<br><br>
								$c_1 = (k_1 L)^2$<br><br>
								$c_2 = (k_2 L)^2$<br><br>
								$L = 2^{precision} - 1$<br><br>
								$MAE(x, y) = mean([l_1, ....., l_N]^T)$<br><br>
								$l_n = |x_n - y_n|$<br><br>
								<div class="fragment fade-in">
									Loss(x, y) = <b class='alert'>M-SSIM</b> = SSIM(x, y) + MAE(x, y)
								</div>
							</div>
						</div>
						
					</section>
					
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Progress on the Validation Set
						</h3>
						<div class="container">
							<img src="assets/lossesBlobsFinder.gif" height="380"/>
						</div>
						<div class="container">
							<img src="assets/trainingBlobsFinder.gif"/>
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Results on Test Set
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/prediction_8.png">
								<img data-src="assets/prediction_15.png">
							</div>
							<div class="col">
								<img data-src="assets/prediction_20.png">
								<img data-src="assets/prediction_84.png">
							</div>
						</div>
						<p style="font-size:20px">Of the 402 simulated in the Test Set, Blobs Finder finds 331 (<b class="alert">~82%</b>)</p>
						<p class="fragment fade-up" style="font-size:20px;">Also a cube is processed in <b class="alert">2.5 ms</b></p>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							There are Cases of <b class="alert">Blended</b> Sources
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/prediction_24.png">
								<img data-src="assets/prediction_69.png">
							</div>
							<div class="col">
								<img data-src="assets/prediction_90.png">
								<p class="fragment fade-up" style="font-size:20px">
									But we are going to take care of that with a combination of <b class='alert'>spectral denoising</b> 
									and <b class="alert">SNR reasoning</b>
								</p>
							</div>
						</div>	
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Spectral Denoising and Lines Detection</h3>
						<img data-src="assets/Pipeline_Spectral.png"/>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;" >
							Feeding the data to<b class="alert"> Deep GRU</b>
						</h3>
						<img data-src="assets/SpectralPreprocessing.png"/>
					</section>
					<section data-background-image="assets/BackToTheFuture.jpg">
						<h3 class="slide-title" style="position:absolut; top:0;">
							<b class="alert">Deep Gated Recurrent Unit </b>
						</h3>
						<div class="container" >
							<div class="col">
								<img data-src="assets/GRU.png">
								<p class="fragment fade-up" style="font-size:18px; background-color:#191919" align="left">
									<b class='alert'>Reset Gate</b>: controls how much of the previous state we want to remember <br>
									$\rightarrow$ captures short-term dependencies in the sequence
								</p>
								<p class="fragment fade-up" style="font-size:18px; background-color:#191919" align="left">
									<b class='alert'>Update Gate</b>: controls how much of the new state is just a copy of the old state<br>
									$\rightarrow$ captures long-term dependencies in the sequence
								</p>
							</div>

							<div class="col">
								<img data-src="assets/DeepGRU.png" style="background-color:#191919">	
								<p class="fragment fade-up" style="font-size:16px; background-color:#191919" align="left" style="margin-left:100px">
								$R_t = \sigma(X_t W_{xr} + H_{t-1}W_{hr} + b_r)$ <br><br>
								$Z_t = \sigma(X_t W_{xz} + H_{t-1}W_{hz} + b_z)$<br>
								$H'_{t - 1} = tanh(X_t W_{xh} + (R_t \times H_{t - 1}W_{hh} + b_h))$<br>
								$\rightarrow$ $R_t \sim 1$ (RNN), $R_t \sim 0$ (MLP)<br><br>
								$H_t = Z?t \times H_{t - 1} + (1 - Z_t) \times H'_{t - 1}$ <br>
								$\rightarrow$ $Z_t \sim 1$ (retain old state), $Z_t \sim 0$ ($H_t \sim H'_{t - 1})<br>
								</p>
							</div>
						</div>
					
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Delving into the <b class="alert">Deep GRU</b>
						</h3>
						<div class="container">
							<div class="col">
								<pre data-id="code-animation">
									<code  data-trim data-line-numbers="1|5-11|12|13-16|18-21">
										class DeepGRU(nn.Module):
											def __init__(self, input_size, 
												hidden_size, output_size):
												super(DeepGRU, self).__init__()
												self.rnn = nn.GRU(
													input_size=input_size,
													hidden_size=hidden_size,
													batch_first=True,
													bidirectional=True,
													dropout=0.1,
												)
												self.num_directions = 2
												self.linear = nn.Linear(
													hidden_size * self.num_directions,
												 	output_size)
												self.act = nn.Sigmoid()

											def forward(self, x):
												pred, hidden = self.rnn(x, None)
												pred = self.act(self.linear(pred))
												return pred
    										

									</code>
								</pre>
							</div>
							<div class="col" align="left" style="font-size:18px">
								$Loss = MAE(x, y) = mean([l_1, ...., l_n]^T)$
								$l_n = |x_n - y_n|$
								<img src="assets/lossesDeepGRU.gif" height="200"/>
							</div>
						</div>
					</section>
					
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Results on Test Set
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/spectral_27.png", height="200">
								<img data-src="assets/spectral_62.png", height="200">
							</div>
							<div class="col">
								<img data-src="assets/spectral_199.png", height="200">
								<img data-src="assets/spectral_285Mod.png", height="200">
							</div>
						</div>
						<p class="fragment fade-down" style="font-size:20px">
							Of the 402 Sources, now we find 394: <b class="alert">$82\% \rightarrow 98\%$</b>
						</p>
						<p class="fragment fade-up" style="font-size:20px">A spectrum is processed in ~<b class="alert">$7 \mu s$</b></p>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Sources are put in <b class="alert">Focus</b> 
						</h3>
						<p><b class='alert'>Focusing</b>: cropping spatially around the source position in the xy plane and integrating only where there is emission</p>
						<div class="container">
							<div class="col">
								<img data-src="assets/focusing_47.png">
							</div>

							<div class="col">
								<img data-src="assets/focusing_63.png">
							</div>
						</div>

					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Sources are put in <b class="alert">Focus</b> 
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/focusing_59.png">
							</div>

							<div class="col">
								<img data-src="assets/focusing_64.png">
							</div>
						</div>

					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Sources are put in <b class="alert">Focus</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/focusing_67.png">
							</div>

							<div class="col">
								<img data-src="assets/focusing_86.png">
							</div>
						</div>

					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Sources are put in <b class="alert">Focus</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/focusing_84.png">
							</div>

							<div class="col">
								<img data-src="assets/focusing_157.png">
							</div>
						</div>

					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Sources are put in <b class="alert">Focus</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/focusing_219.png">
							</div>

							<div class="col">
								<img data-src="assets/focusing_421.png">
							</div>
						</div>

					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Sources are put in <b class="alert">Focus</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/focusing_411.png">
							</div>

							<div class="col">
								<img data-src="assets/focusing_446.png">
							</div>
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Gain in <b class="alert">SNR</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/snrboost.png">
							</div>

							<div class="col">
								<p class="fragment fade-up" style="font-size:20px">
									It's time to feed the <b class='alert'>Focused</b> sources to the <b class="alert">ResNets</b>
								</p>
								<img class="fragment fade-in" data-src="assets/ResNetInput.png"/>
				
							</div>
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">Sources Parameters Estimation</h3>
						<img data-src="assets/Pipeline_Parameters.png"/>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							The <b class="alert">ResNet Block</b>
						</h3>
						<div class="container">
							<div class="col">
								<img data-src="assets/ResBlock.png"/>
							</div>
							<div class="col">
								<pre data-id="code-animation">
									<code data-trim data-line-numbers="1|4|5-8|9-13|14|15-18|19|21-24|28-33">
										class ResBlock(nn.Module):
    										def __init__(self, in_channels, out_channels, downsample):
    										    super().__init__()
    										    if downsample:
    										        self.conv1 = nn.Conv2d(
    										            in_channels, out_channels, 
														kernel_size=3, stride=2, padding=1
    										        )
    										        self.shortcut = nn.Sequential(
    										            nn.Conv2d(in_channels, out_channels, 
														kernel_size=1, stride=2),
    										            nn.BatchNorm2d(out_channels),
    										        )
    										    else:
    										        self.conv1 = nn.Conv2d(
    										            in_channels, out_channels, 
														kernel_size=3, stride=1, padding=1
    										        )
    										        self.shortcut = nn.Sequential()

    										    self.conv2 = nn.Conv2d(
    										        out_channels, out_channels, 
													kernel_size=3, stride=1, padding=1
    										    )
    										    self.bn1 = nn.BatchNorm2d(out_channels)
    										    self.bn2 = nn.BatchNorm2d(out_channels)

    										def forward(self, x):
    										    shortcut = self.shortcut(x)
    										    x = nn.ReLU()(self.bn1(self.conv1(x)))
    										    x = nn.ReLU()(self.bn2(self.conv2(x)))
    										    x = x + shortcut
    										    return nn.ReLU()(x)
									</code>
								</pre>

							</div>
						</div>
					</section>
					
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Getting to know the <b class="alert">ResNet</b>
						</h3>
						<div class="container">
							<div class="col">
								<img src="assets/ResNet.png" height="650">
							</div>
							<div class="col">
								<pre data-id="code-animation">
									<code data-trim data-line-numbers="1|8-13|15-18|20-23|25-28|30-33|35|36-41|43-52">
										class ResNet18(nn.Module):
    										def __init__(self, c_in, c_out):
    										    super(ResNet18, self).__init__()

    										    self.c_in = c_in
    										    self.c_out = c_out

    										    self.layer0 = nn.Sequential(
    										        nn.Conv2d(self.c_in, 64, kernel_size=3, stride=1, padding=1),
    										        nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
    										        nn.BatchNorm2d(64),
    										        nn.ReLU(),
    										    )

    										    self.layer1 = nn.Sequential(
    										        blocks.ResBlock(64, 64, downsample=False),
    										        blocks.ResBlock(64, 64, downsample=False),
    										    )

    										    self.layer2 = nn.Sequential(
    										        blocks.ResBlock(64, 128, downsample=True),
    										        blocks.ResBlock(128, 128, downsample=True),
    										    )

    										    self.layer3 = nn.Sequential(
    										        blocks.ResBlock(128, 256, downsample=True),
    										        blocks.ResBlock(256, 256, downsample=False),
    										    )

    										    self.layer4 = nn.Sequential(
    										        blocks.ResBlock(256, 512, downsample=True),
    										        blocks.ResBlock(512, 512, downsample=False),
    										    )

    										    self.gap = torch.nn.AdaptiveMaxPool2d(1)
    										    self.fc = nn.Sequential(
    										        nn.Linear(512, 128, bias=True),
    										        nn.ReLU(),
    										        nn.Linear(128, self.c_out),
    										        # nn.Sigmoid()
    										    )

    										def forward(self, x):
    										    x = self.layer0(x)
    										    x = self.layer1(x)
    										    x = self.layer2(x)
    										    x = self.layer3(x)
    										    x = self.layer4(x)
    										    x = self.gap(x)
    										    x = torch.flatten(x, start_dim=1)
    										    x = self.fc(x)
    										    return x
									</code>
								</pre>
							</div>
						
						</div>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Results on Test Set
						</h3>

						<div class="container">
							<img data-src="assets/parameters_17.png"/>
							<table style="font-size:16px; position: relative; top:200px ; left:-440px;">
								<thead>
									<tr>
										<th>Parameter&emsp;&emsp;&emsp;</th>
										<th>Mean</th>
										<th>Stddev</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td>Res $x$</td>
										<td>$0.20$</td>
										<td>$0.60$</td>
									</tr>
									<tr>
										<td>Res $y$</td>
										<td>$0.12$</td>
										<td>$0.33$</td>
									</tr>
									<tr>
										<td>Res $FWHM_x$</td>
										<td>$0.02$</td>
										<td>$0.47$</td>
									</tr>
									<tr>
										<td>Res $FWHM_y$</td>
										<td>$0.13$</td>
										<td>$0.82$</td>
									</tr>
									<tr>
										<td>Res $FWHM_y$</td>
										<td>$0.00$</td>
										<td>$0.03$</td>
									</tr>
								</tbody>
							</table>
								
						</div>

					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							Model Image Generation from <b class="alert">Parameters</b>
						</h3>
						<img src="assets/ResNetGeneratedModel.png"/>
						<p style=" position: relative; bottom:200px; margin-left:40px" align="left">Processing time per image <b class="alert">0.5 ms</b></p>
					</section>

					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							3D Model and <b class="alert">Compression</b>
						</h3>
						<img src="assets/3dModelAndCompression.png"/>
						<p class="fragment fade-in" style=" position: relative; bottom:100px; left:500px; margin-left:40px" align="left">
							A model cube weights  <b class="alert">67.2 MBs</b>, <br>
							while the parameters to generate it (assuming 5 sources inside) only <br>
							<b class="alert">1.39 KBs</b>
						</p>
						<p class="fragment fade-in" style=" position: relative; bottom:100px; left:500px; margin-left:40px" align="left">
							We have achieved a mindbugging compression factor of <b class="alert">99.9%</b>
						</p>
						<p class="fragment fade-in" style=" position: relative; bottom:100px; left:500px; margin-left:40px" align="left">
							Our skymodel test set of ~12.50 GBs can be re-generated from a .csv <br> of 278 KBs. 
						</p>
					</section>		
				</section>
				<!-- Fifth set of slices - Future Work and Conclusions -->
				<section>
					<section  data-background="black">
						<h3 class="slide-title" style="position:absolut; top:0;">
							<b class="alert">Future Work</b>
						</h3>
						<div class="container" style="margin-left:20px">
							<div class="col" style="font-size:20px">
								<ul>
									<li>
										<input type="checkbox" checked>Show our work at the AIForum<br>
										<input type="checkbox">Submit the paper to MNRAS
									</li>
									<br>
									<li class="fragment fade-in">Generate more <b class="alert">realistic</b> simulations:<br>
										<input type="checkbox" 
										checked>Introduce spectral index $\rightarrow$ better continuum<br>
										<input type="checkbox">Introduce velocity dispersion and 3D inclination<br>
										<input type="checkbox">Model complex line profiles (velocity and inclination degeneracy)<br>
										<input type="checkbox">Introduce complex morphologies and sources classes
									</li>
									<br>
									<li class="fragment fade-in">
										<input type="checkbox">Study the pipeline performances with decreasing source SNR (find the limit)
									</li>
									<br>
									<li class="fragment fade-in">
										<input type="checkbox">Apply the pipeline to real ALMA observations and compare the performance with <br>
										similar taks in CASA
									</li>
									<br>
									<li class="fragment fade-in">
										<input type="checkbox">Uncertainties propagation
									</li>
									<br>
									<li class="fragment fade-in">
										<input type="checkbox">Investigate the potential speed-up of cube deconvolution
									</li>
									<br>
								</ul>
								
							</div>
							<div class="col">
								<img src="assets/WeBrakeForNobody.jpeg">
								<img src="assets/SeeYouInSpace.png" height="100px">
							</div>
	
						</div>
	
					</section>
					<section>
						<h3 class="slide-title" style="position:absolut; top:0;">
							<b class="alert">Takeaway messages</b>
						</h3>
						<ul>
							<li class="fragment fade-in">
								We successfully detect 98% of the sources in the data and measure their morphological parameters within subpixel accuracies
							</li>
							<br>
							<li class="fragment fade-in">
								Frequency information is crucial to confirm sources and integrating over the full frequency range may cover faint sources
							</li>
							<br>
							<li class="fragment fade-in">
								The pipeline processes a full data-cube in ~7 ms
							</li>
							<br>
							<li class="fragment fade-in">
								If only parameters are stored, the pipeline achieves compression rates that could be used to improve data sharing and storing with current facilities
							</li>
							<li class="fragment fade-in">
								There is still a lot of work to be done.
							</li>
						</ul>
					</section>

					<section data-background="#fafafa">
						<h1 class="slide-title" style="position:absolut; top:0; color: black">
							Thank you for your Attention
						</h1>
						<div class="container">
							<div class="col">
								<img src="assets/SpaceSloth.png" height="150" align="left" style="margin-left:80px">
								<ul style="color: black">
									<li>micheledelliveneri@gmail.com</li>
									<li>michele.delliveneri@na.infn.it</li>
									<li>michele.delliveneri@unina.it</li>
									<li>@m_delliveneri</li>
									
								</ul>


							</div>

							<div class="col">
								<img src="assets/theAnswer.png"/>
							</div>
						</div>
					</section>
				
	
				</section>
				


			
			</div>
		</div>

		<style>
			.reveal .block {
				background-color: #191919;
				margin-left: 20px;
				margin-right: 20px;
				text-align: left;
				padding-bottom: 0.1em;
			}
	
			.reveal .block-title {
				background-color: #333333;
				padding: 8px 35px 8px 14px;
				color: #FFAA7F;
				font-weight: bold;
			}
	
			.reveal .block-content {
				padding: 8px 35px 8px 14px;
			}
	
			.reveal .slide-title {
				border-left: 5px solid white;
				text-align: left;
				margin-left: 20px;
				padding-left: 20px;
			}
	
			.reveal .alert {
				color: #FFAA7F;
				font-weight: bold;
			}
	
			.reveal .inverted {
				filter: invert(100%);
			}

			.container{
			  display: flex;
			}
			.col {
			  flex: 1;
			}
		</style>

		<script src="reveal.js/dist/reveal.js"></script>
		<script src="reveal.js/plugin/notes/notes.js"></script>
		<script src="reveal.js/plugin/markdown/markdown.js"></script>
		<script src="reveal.js/plugin/highlight/highlight.js"></script>
		<script src="reveal.js/plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				controls: true,
				hash: true,
				controlBackArrows: "hidden",
				progress: true, 
				slideNumber: true,
				transition: "slide",
				widht: 1280,
				height: 720,
				margin: 0.1,
				minScale: 0.2,
				maxScale: 1.5,

				autoplayMedia: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

				dependencies: [
					{
						src: "reveal.js/plugin/markdown/marked.js"
					},
					{
						src: "reveal.js/plugin/markdown/markdown.js"
					},
					{
						src: "reveal.js/plugin/notes/notes.js",
						async: true
					},
					{
						src: 'reveal.js/plugin/math/math.js',
						async: true
					},
					{
						src: "reveal.js/plugin/reveal.js-d3/reveald3.js"
					},
					{
						src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
					},
					{
						src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
					},
					{
						src: 'reveal.js/plugin/highlight/highlight.js',
						async: true
					},
				]


			});

		</script>
	</body>
</html>
